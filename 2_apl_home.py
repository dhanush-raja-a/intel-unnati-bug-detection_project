# -*- coding: utf-8 -*-
"""2 apl home.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Sh2JJ1sBURu87LeyUqmiOUg6vDOV7x5a
"""

pip install datasets pandas

!pip install unsloth

from datasets import load_dataset
import pandas as pd

# Load the dataset
dataset = load_dataset("alexjercan/bugnet")

# Convert to DataFrame
df = pd.DataFrame(dataset['train'])  # Adjust split as necessary

# Display available columns to understand the dataset structure
print(df.columns)

# Filter for specific languages if a 'language' column exists
if 'language' in df.columns:
    languages = ["python", "c++"]  # Adjust based on available data
    df_filtered = df[df['language'].str.lower().isin(languages)]
else:
    df_filtered = df  # Use the dataset as is if no language column exists

# Save to CSV
df_filtered.to_csv("buggy_fixed_code_dataset.csv", index=False)

print("Dataset saved as buggy_fixed_code_dataset.csv")

from datasets import load_dataset
import pandas as pd

# List of languages to download
languages = ["python", "js", "java", "go", "cpp", "rust"]

for lang in languages:
    # Load the dataset for the specified language with trust_remote_code=True
    dataset = load_dataset("bigcode/humanevalpack", lang, split="test", trust_remote_code=True)

    # Convert the dataset to a pandas DataFrame
    df = pd.DataFrame(dataset)

    # Save the DataFrame to a CSV file
    csv_filename = f"humanevalpack_{lang}.csv"
    df.to_csv(csv_filename, index=False)
    print(f"Saved {lang} dataset to {csv_filename}")

import pandas as pd
import json

# Load the CSV file
df = pd.read_csv("/content/buggy_fixed_code_dataset.csv")
df.head()

import pandas as pd
import json

# Load the CSV file
df = pd.read_csv("/content/buggy_fixed_code_dataset.csv")

# Ensure the column names match your dataset


# Convert CSV data into JSON conversation format
data = []
for _, row in df.iterrows():
    conversation = {
        "conversations": [
            {"role": "system", "content": "You are an AI that fixes all programming language  code."},
            {"role": "user", "content": f"Buggy Code:\n{row['fail']}\n\n"},
            {"role": "assistant", "content": row['pass']}
        ]
    }
    data.append(conversation)

# Save formatted data to a JSON file
output_file = "/content/formatted_datasetfordataset_buggy_fixed_code_dataset.json"
with open(output_file, "w", encoding="utf-8") as f:
    json.dump(data, f, indent=4)

print(f"Formatted dataset saved to: {output_file}")

# Preview the first few entries
with open(output_file, "r", encoding="utf-8") as f:
    preview_data = json.load(f)
    print(json.dumps(preview_data[:3], indent=4))

import pandas as pd
import json
import glob

# File paths for all datasets
dataset_paths = glob.glob("/content/humanevalpack_*.csv") + ["/content/buggy_fixed_code_dataset.csv"]

# Combined dataset
combined_data = []

for file_path in dataset_paths:
    # Load the CSV file
    df = pd.read_csv(file_path)

    # Determine the column names dynamically
    if "fail" in df.columns and "pass" in df.columns:
        # Format for buggy_fixed_code_dataset.csv
        for _, row in df.iterrows():
            conversation = {
                "conversations": [
                    {"role": "system", "content": "You are a code-fixing assistant. You have expertise in all programming languages and can debug and correct any code."},
                    {"role": "user", "content": f"Buggy Code:\n{row['fail']}"},
                    {"role": "assistant", "content": row['pass']}
                ]
            }
            combined_data.append(conversation)
    elif "buggy_solution" in df.columns and "canonical_solution" in df.columns:
        # Format for humanevalpack datasets
        for _, row in df.iterrows():
            conversation = {
                "conversations": [
                    {"role": "system", "content": "You are a code-fixing assistant. You have expertise in all programming languages and can debug and correct any code."},
                    {"role": "user", "content": f"Buggy Code:\n{row['buggy_solution']}"},
                    {"role": "assistant", "content": row['canonical_solution']}
                ]
            }
            combined_data.append(conversation)

# Save the combined formatted data to a single JSON file
output_file = "/content/combined_formatted_dataset.json"
with open(output_file, "w", encoding="utf-8") as f:
    json.dump(combined_data, f, indent=4)

print(f"Combined dataset saved to: {output_file}")

import json

# Path to the combined dataset
file_path = "/content/combined_formatted_dataset.json"

# Load the dataset
with open(file_path, "r", encoding="utf-8") as f:
    data = json.load(f)

# Print sample data
print(json.dumps(data[:3], indent=4))  # Show first 3 examples

from unsloth import FastLanguageModel
import torch
max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!
dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+
load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.
# More models at https://huggingface.co/unsloth

model, tokenizer = FastLanguageModel.from_pretrained(
    model_name = "unsloth/Llama-3.2-3B-Instruct", # or choose "unsloth/Llama-3.2-1B-Instruct"
    max_seq_length = max_seq_length,
    dtype = dtype,
    load_in_4bit = load_in_4bit,
    # token = "hf_...", # use one if using gated models like meta-llama/Llama-2-7b-hf
)

model = FastLanguageModel.get_peft_model(
    model,
    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128
    target_modules = ["q_proj", "k_proj", "v_proj", "o_proj",
                      "gate_proj", "up_proj", "down_proj",],
    lora_alpha = 16,
    lora_dropout = 0, # Supports any, but = 0 is optimized
    bias = "none",    # Supports any, but = "none" is optimized
    # [NEW] "unsloth" uses 30% less VRAM, fits 2x larger batch sizes!
    use_gradient_checkpointing = "unsloth", # True or "unsloth" for very long context
    random_state = 3407,
    use_rslora = False,  # We support rank stabilized LoRA
    loftq_config = None, # And LoftQ
)

import json
from datasets import load_dataset
from unsloth.chat_templates import standardize_sharegpt, get_chat_template

import json
from datasets import load_dataset
from unsloth.chat_templates import standardize_sharegpt, get_chat_template

# Load JSONL dataset
dataset_path = "/content/combined_formatted_dataset.json"
dataset = load_dataset("json", data_files=dataset_path, split="train")

# Standardize dataset using Unsloth's ShareGPT function
dataset = standardize_sharegpt(dataset)

# Initialize tokenizer with the correct chat template
from transformers import AutoTokenizer

model_name = "unsloth/Llama-3-8B-Instruct"  # Change to your model
tokenizer = AutoTokenizer.from_pretrained(model_name)

tokenizer = get_chat_template(
    tokenizer,
    chat_template="llama-3.1",
)

# Define formatting function
def formatting_prompts_func(examples):
    convos = examples["conversations"]
    texts = [tokenizer.apply_chat_template(convo, tokenize=False, add_generation_prompt=False) for convo in convos]
    return {"text": texts}

# Apply formatting
dataset = dataset.map(formatting_prompts_func, batched=True)

# Save the processed dataset
dataset.to_json("formatted_dataset_for_dataset.jsonl")

dataset[0]

dataset[5]["conversations"]

dataset[5]["text"]

from trl import SFTTrainer
from transformers import TrainingArguments, DataCollatorForSeq2Seq
from unsloth import is_bfloat16_supported

trainer = SFTTrainer(
    model = model,
    tokenizer = tokenizer,
    train_dataset = dataset,
    dataset_text_field = "text",
    max_seq_length = max_seq_length,
    data_collator = DataCollatorForSeq2Seq(tokenizer = tokenizer),
    dataset_num_proc = 2,
    packing = False, # Can make training 5x faster for short sequences.
    args = TrainingArguments(
        per_device_train_batch_size = 2,
        gradient_accumulation_steps = 4,
        warmup_steps = 5,
        # num_train_epochs = 1, # Set this for 1 full training run.
        max_steps = 20,
        learning_rate = 2e-4,
        fp16 = not is_bfloat16_supported(),
        bf16 = is_bfloat16_supported(),
        logging_steps = 1,
        optim = "adamw_8bit",
        weight_decay = 0.01,
        lr_scheduler_type = "linear",
        seed = 3407,
        output_dir = "outputs",
        report_to = "none", # Use this for WandB etc
    ),
)

from unsloth.chat_templates import train_on_responses_only
trainer = train_on_responses_only(
    trainer,
    instruction_part = "<|start_header_id|>user<|end_header_id|>\n\n",
    response_part = "<|start_header_id|>assistant<|end_header_id|>\n\n",
)

tokenizer.decode(trainer.train_dataset[5]["input_ids"])

space = tokenizer(" ", add_special_tokens = False).input_ids[0]
tokenizer.decode([space if x == -100 else x for x in trainer.train_dataset[5]["labels"]])

trainer_stats = trainer.train()

FastLanguageModel.for_inference(model) # Enable native 2x faster inference

messages = [
    {"role": "user", "content": """Fix the following java code:# Fix the following java code:
# Fix the following Python code:


public class MathOperations {
    public static void main(String[] args) {
        int result = divide(10, 0); // Division by zero error
        System.out.println("Result: " + result);

        System.out.println("Factorial of 5: " + factorial(5));
    }


    public static int divide(int a, int b) {
        return a / b; // No check for division by zero
    }


    public static int factorial(int n) {
        if (n = 0) { // Assignment instead of comparison
            return 1;
        } else {
            return n * factorial(n - 1);
        }
    }
}
"""},
]
inputs = tokenizer.apply_chat_template(
    messages,
    tokenize = True,
    add_generation_prompt = True, # Must add for generation
    return_tensors = "pt",
).to("cuda")

from transformers import TextStreamer
text_streamer = TextStreamer(tokenizer, skip_prompt = True)
_ = model.generate(input_ids = inputs, streamer = text_streamer, max_new_tokens = 1200,
                   use_cache = True, temperature = 1.5, min_p = 0.1)

# Define conversation prompt
messages = [
    {"role": "system", "content": "You are an AI that fixes Java code efficiently."},
    {"role": "user", "content": """Fix the following Java code:

public class MathOperations {
    public static void main(String[] args) {
        int result = divide(10, 0); // Division by zero error
        System.out.println("Result: " + result);

        System.out.println("Factorial of 5: " + factorial(5));
    }

    public static int divide(int a, int b) {
        return a / b; // No check for division by zero
    }

    public static int factorial(int n) {
        if (n = 0) { // Assignment instead of comparison
            return 1;
        } else {
            return n * factorial(n - 1);
        }
    }
}
"""}
]

# Tokenize input
inputs = tokenizer.apply_chat_template(
    messages,
    tokenize=True,
    add_generation_prompt=True,
    return_tensors="pt"
).to("cuda")

# Add explicit attention mask
attention_mask = torch.ones(inputs.shape, dtype=torch.long).to("cuda")

# Set up streaming
text_streamer = TextStreamer(tokenizer, skip_prompt=True)

# Generate corrected Java code
_ = model.generate(
    input_ids=inputs,
    attention_mask=attention_mask,  # Ensure reliable generation
    streamer=text_streamer,
    max_new_tokens=1200,
    use_cache=True,
    temperature=1.5,
    min_p=0.1
)

model.save_pretrained("lora_model")  # Local saving
tokenizer.save_pretrained("lora_model")

if False:
    from unsloth import FastLanguageModel
    model, tokenizer = FastLanguageModel.from_pretrained(
        model_name = "lora_model", # YOUR MODEL YOU USED FOR TRAINING
        max_seq_length = max_seq_length,
        dtype = dtype,
        load_in_4bit = load_in_4bit,
    )
    FastLanguageModel.for_inference(model) # Enable native 2x faster inference

messages = [
    {"role": "user", "content": "print('hellow)"},
]
inputs = tokenizer.apply_chat_template(
    messages,
    tokenize = True,
    add_generation_prompt = True, # Must add for generation
    return_tensors = "pt",
).to("cuda")

from transformers import TextStreamer
text_streamer = TextStreamer(tokenizer, skip_prompt = True)
_ = model.generate(input_ids = inputs, streamer = text_streamer, max_new_tokens = 128,
                   use_cache = True, temperature = 1.5, min_p = 0.1)

from google.colab import files
import shutil

# Create a zip file of the model directory
shutil.make_archive("lora_model", 'zip', "lora_model")

# Download the zipped model
files.download("lora_model.zip")

